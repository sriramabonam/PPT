{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC6FjPi0nSns"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def get_hadoop_components(file_path):\n",
        "    components = []\n",
        "    try:\n",
        "        tree = ET.parse(file_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Find the property elements with name \"fs.defaultFS\"\n",
        "        for property_elem in root.iter('property'):\n",
        "            name_elem = property_elem.find('name')\n",
        "            value_elem = property_elem.find('value')\n",
        "\n",
        "            if name_elem is not None and value_elem is not None:\n",
        "                name = name_elem.text.strip()\n",
        "                value = value_elem.text.strip()\n",
        "\n",
        "                if name == \"fs.defaultFS\":\n",
        "                    # Extract the hostname from the value\n",
        "                    components.append(value.split(\"://\")[1])\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{file_path}' not found.\")\n",
        "    except ET.ParseError:\n",
        "        print(f\"Error parsing the file '{file_path}'.\")\n",
        "\n",
        "    return components\n",
        "\n",
        "\n",
        "# Provide the path to your core-site.xml file\n",
        "config_file_path = \"/path/to/core-site.xml\"\n",
        "\n",
        "components = get_hadoop_components(config_file_path)\n",
        "\n",
        "if components:\n",
        "    print(\"Core components of Hadoop:\")\n",
        "    for component in components:\n",
        "        print(component)\n",
        "else:\n",
        "    print(\"No core components found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hdfs import InsecureClient\n",
        "\n",
        "def calculate_directory_size(hdfs_host, hdfs_port, hdfs_user, directory_path):\n",
        "    # Create an HDFS client\n",
        "    client = InsecureClient(f\"http://{hdfs_host}:{hdfs_port}\", user=hdfs_user)\n",
        "\n",
        "    total_size = 0\n",
        "\n",
        "    # Iterate over files in the directory\n",
        "    for file_info in client.list(directory_path, status=True):\n",
        "        file_path = file_info['path']\n",
        "        if file_info['type'] == 'FILE':\n",
        "            # Get the size of the file\n",
        "            file_size = file_info['length']\n",
        "            total_size += file_size\n",
        "\n",
        "    return total_size\n"
      ],
      "metadata": {
        "id": "bPCUQFtbneLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import heapq\n",
        "\n",
        "\n",
        "class TopNWords(MRJob):\n",
        "    def configure_args(self):\n",
        "        super(TopNWords, self).configure_args()\n",
        "        self.add_passthru_arg('--top', type=int, help='Number of top words to display')\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_get_words,\n",
        "                   combiner=self.combiner_count_words,\n",
        "                   reducer=self.reducer_count_words),\n",
        "            MRStep(reducer=self.reducer_find_top_n_words)\n",
        "        ]\n",
        "\n",
        "    def mapper_get_words(self, _, line):\n",
        "        # Split the line into words and yield each word\n",
        "        for word in line.split():\n",
        "            yield word.lower(), 1\n",
        "\n",
        "    def combiner_count_words(self, word, counts):\n",
        "        # Sum the counts of each word\n",
        "        yield word, sum(counts)\n",
        "\n",
        "    def reducer_count_words(self, word, counts):\n",
        "        # Sum the counts of each word\n",
        "        yield word, sum(counts)\n",
        "\n",
        "    def reducer_find_top_n_words(self, word, counts):\n",
        "        # Create a heap of size N to keep track of the top N words\n",
        "        top_n = self.options.top\n",
        "        heap = [(count, word) for count, word in counts]\n",
        "        top_words = heapq.nlargest(top_n, heap)\n",
        "\n",
        "        # Yield each top word with its count\n",
        "        for count, word in top_words:\n",
        "            yield word, count\n",
        "\n",
        "    def mapper_final(self):\n",
        "        # Final mapper to sort the output\n",
        "        for word, count in self.reducer_find_top_n_words():\n",
        "            yield None, (count, word)\n",
        "\n",
        "    def reducer_final(self, _, counts):\n",
        "        # Final reducer to sort the output and display the top N words\n",
        "        top_n = self.options.top\n",
        "        heap = [(count, word) for count, word in counts]\n",
        "        top_words = heapq.nlargest(top_n, heap)\n",
        "\n",
        "        # Display the top N words\n",
        "        for count, word in top_words:\n",
        "            print(f'{word}: {count}')\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_get_words,\n",
        "                   combiner=self.combiner_count_words,\n",
        "                   reducer=self.reducer_count_words),\n",
        "            MRStep(reducer=self.reducer_find_top_n_words),\n",
        "            MRStep(mapper_final=self.mapper_final,\n",
        "                   reducer_final=self.reducer_final)\n",
        "        ]\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    TopNWords.run()\n"
      ],
      "metadata": {
        "id": "0Ugv5DXAnm51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def check_namenode_health(namenode_url):\n",
        "    # Send a GET request to the NameNode's JMX endpoint\n",
        "    url = f\"{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        live_nodes = data['beans'][0]['LiveNodes']\n",
        "        dead_nodes = data['beans'][0]['DeadNodes']\n",
        "\n",
        "        # Check if the NameNode is in SafeMode\n",
        "        safemode = data['beans'][0]['Safemode']\n",
        "\n",
        "        if safemode:\n",
        "            print(\"NameNode is in SafeMode.\")\n",
        "        else:\n",
        "            print(\"NameNode is active.\")\n",
        "\n",
        "        print(f\"Live Nodes: {live_nodes}\")\n",
        "        print(f\"Dead Nodes: {dead_nodes}\")\n",
        "    else:\n",
        "        print(\"Error checking NameNode health.\")\n",
        "\n",
        "def check_datanode_health(namenode_url):\n",
        "    # Send a GET request to the NameNode's JMX endpoint\n",
        "    url = f\"{namenode_url}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        datanode_count = data['beans'][0]['NumLiveDataNodes']\n",
        "\n",
        "        print(f\"Number of live DataNodes: {datanode_count}\")\n",
        "    else:\n",
        "        print(\"Error checking DataNode health.\")\n",
        "\n",
        "# Provide the URL of the NameNode's web UI\n",
        "namenode_url = \"http://localhost:9870\"\n",
        "\n",
        "# Check NameNode health\n",
        "check_namenode_health(namenode_url)\n",
        "\n",
        "# Check DataNode health\n",
        "check_datanode_health(namenode_url)\n"
      ],
      "metadata": {
        "id": "kQ62hJn0n0xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hdfs import InsecureClient\n",
        "\n",
        "def list_hdfs_path(hdfs_host, hdfs_port, hdfs_user, hdfs_path):\n",
        "    # Create an HDFS client\n",
        "    client = InsecureClient(f\"http://{hdfs_host}:{hdfs_port}\", user=hdfs_user)\n",
        "\n",
        "    # List all files and directories in the given HDFS path\n",
        "    contents = client.list(hdfs_path, status=True)\n",
        "\n",
        "    # Print the file and directory names\n",
        "    for item in contents:\n",
        "        item_type = item['type']\n",
        "        item_name = item['path']\n",
        "\n",
        "        if item_type == 'DIRECTORY':\n",
        "            print(f\"[DIRECTORY] {item_name}\")\n",
        "        elif item_type == 'FILE':\n",
        "            print(f\"[FILE] {item_name}\")\n",
        "\n",
        "    return contents\n",
        "\n",
        "\n",
        "# Provide the HDFS connection details\n",
        "hdfs_host = 'localhost'\n",
        "hdfs_port = 50070\n",
        "hdfs_user = 'hadoop'\n",
        "\n",
        "# Provide the HDFS path to list\n",
        "hdfs_path = '/user/hadoop/data'\n",
        "\n",
        "# List the files and directories in the HDFS path\n",
        "list_hdfs_path(hdfs_host, hdfs_port, hdfs_user, hdfs_path)\n"
      ],
      "metadata": {
        "id": "gPooLp7_oR7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hdfs import InsecureClient\n",
        "\n",
        "def analyze_storage_utilization(hdfs_host, hdfs_port, hdfs_user):\n",
        "    # Create an HDFS client\n",
        "    client = InsecureClient(f\"http://{hdfs_host}:{hdfs_port}\", user=hdfs_user)\n",
        "\n",
        "    # Get the storage utilization information for all DataNodes\n",
        "    datanodes = client.list('/datanodes', status=True)\n",
        "\n",
        "    # Analyze storage utilization\n",
        "    max_storage_node = None\n",
        "    max_storage_capacity = 0\n",
        "    min_storage_node = None\n",
        "    min_storage_capacity = float('inf')\n",
        "\n",
        "    for datanode in datanodes:\n",
        "        node_name = datanode['path']\n",
        "        storage_capacity = datanode['length']\n",
        "\n",
        "        if storage_capacity > max_storage_capacity:\n",
        "            max_storage_capacity = storage_capacity\n",
        "            max_storage_node = node_name\n",
        "\n",
        "        if storage_capacity < min_storage_capacity:\n",
        "            min_storage_capacity = storage_capacity\n",
        "            min_storage_node = node_name\n",
        "\n",
        "    # Display the results\n",
        "    print(\"Storage Utilization Analysis:\")\n",
        "    print(f\"Highest Storage Capacity: {max_storage_node} - {max_storage_capacity} bytes\")\n",
        "    print(f\"Lowest Storage Capacity: {min_storage_node} - {min_storage_capacity} bytes\")\n",
        "\n",
        "\n",
        "# Provide the HDFS connection details\n",
        "hdfs_host = 'localhost'\n",
        "hdfs_port = 50070\n",
        "hdfs_user = 'hadoop'\n",
        "\n",
        "# Analyze storage utilization of DataNodes\n",
        "analyze_storage_utilization(hdfs_host, hdfs_port, hdfs_user)\n"
      ],
      "metadata": {
        "id": "43IqhMlroTTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "def submit_yarn_job(resource_manager_url, job_name, jar_file, main_class, input_path, output_path):\n",
        "    # Submit the Hadoop job to the YARN ResourceManager\n",
        "    url = f\"{resource_manager_url}/ws/v1/cluster/apps/new-application\"\n",
        "    response = requests.post(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        application_id = data[\"application-id\"]\n",
        "\n",
        "        # Submit the job configuration\n",
        "        job_submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/job\"\n",
        "        payload = {\n",
        "            \"application-id\": application_id,\n",
        "            \"application-name\": job_name,\n",
        "            \"application-type\": \"MAPREDUCE\",\n",
        "            \"am-container-spec\": {\n",
        "                \"commands\": {\n",
        "                    \"command\": f\"hadoop jar {jar_file} {main_class} {input_path} {output_path}\"\n",
        "                }\n",
        "            },\n",
        "            \"kerberos-principal\": \"your_principal\",\n",
        "            \"max-app-attempts\": 2,\n",
        "            \"resource\": {\n",
        "                \"memory\": 1024,\n",
        "                \"vCores\": 1\n",
        "            },\n",
        "            \"queue\": \"default\",\n",
        "            \"tags\": []\n",
        "        }\n",
        "        response = requests.post(job_submit_url, json=payload)\n",
        "\n",
        "        if response.status_code == 202:\n",
        "            print(\"Job submitted successfully.\")\n",
        "            return application_id\n",
        "        else:\n",
        "            print(\"Error submitting job.\")\n",
        "    else:\n",
        "        print(\"Error creating new application.\")\n",
        "\n",
        "    return None\n",
        "\n",
        "def monitor_job_progress(resource_manager_url, application_id):\n",
        "    # Monitor the progress of the job\n",
        "    job_info_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}\"\n",
        "\n",
        "    while True:\n",
        "        response = requests.get(job_info_url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            state = data[\"app\"][\"state\"]\n",
        "            progress = data[\"app\"][\"progress\"]\n",
        "\n",
        "            print(f\"Job state: {state}\")\n",
        "            print(f\"Job progress: {progress}%\")\n",
        "\n",
        "            if state == \"FINISHED\":\n",
        "                print(\"Job finished.\")\n",
        "                break\n",
        "            elif state in [\"KILLED\", \"FAILED\"]:\n",
        "                print(\"Job failed or terminated.\")\n",
        "                break\n",
        "\n",
        "        time.sleep(5)\n",
        "\n",
        "def retrieve_output(resource_manager_url, application_id):\n",
        "    # Retrieve the final output of the job\n",
        "    output_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/job/output\"\n",
        "    response = requests.get(output_url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        output_path = data[\"jobOutput\"][\"outputPath\"]\n",
        "\n",
        "        print(f\"Output path: {output_path}\")\n",
        "    else:\n",
        "        print(\"Error retrieving job output.\")\n",
        "\n",
        "# Provide the ResourceManager URL\n",
        "resource_manager_url = \"http://localhost:8088\"\n",
        "\n",
        "# Provide the details of the job to be submitted\n",
        "job_name = \"MyHadoopJob\"\n",
        "jar_file = \"path_to_your_hadoop_jar_file\"\n",
        "main_class = \"your_hadoop_main_class\"\n",
        "input_path = \"input_path_in_hdfs\"\n",
        "output_path = \"output_path_in_hdfs\"\n",
        "\n",
        "# Submit the Hadoop job to YARN\n",
        "application_id = submit_yarn_job(resource_manager_url, job_name, jar_file, main_class, input_path, output_path)\n",
        "\n",
        "if application_id:\n",
        "    # Monitor the progress of the job\n",
        "    monitor_job_progress(resource_manager_url, application_id)\n",
        "\n",
        "    # Retrieve the final output of the job\n",
        "    retrieve_output(resource_manager_url, application_id)\n"
      ],
      "metadata": {
        "id": "-fSeot-rodgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "def submit_yarn_job(resource_manager_url, job_name, jar_file, main_class, input_path, output_path, memory_mb, vcores):\n",
        "    # Submit the Hadoop job to the YARN ResourceManager\n",
        "    url = f\"{resource_manager_url}/ws/v1/cluster/apps/new-application\"\n",
        "    response = requests.post(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        application_id = data[\"application-id\"]\n",
        "\n",
        "        # Submit the job configuration\n",
        "        job_submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/job\"\n",
        "        payload = {\n",
        "            \"application-id\": application_id,\n",
        "            \"application-name\": job_name,\n",
        "            \"application-type\": \"MAPREDUCE\",\n",
        "            \"am-container-spec\": {\n",
        "                \"commands\": {\n",
        "                    \"command\": f\"hadoop jar {jar_file} {main_class} {input_path} {output_path}\"\n",
        "                }\n",
        "            },\n",
        "            \"kerberos-principal\": \"your_principal\",\n",
        "            \"max-app-attempts\": 2,\n",
        "            \"resource\": {\n",
        "                \"memory\": memory_mb,\n",
        "                \"vCores\": vcores\n",
        "            },\n",
        "            \"queue\": \"default\",\n",
        "            \"tags\": []\n",
        "        }\n",
        "        response = requests.post(job_submit_url, json=payload)\n",
        "\n",
        "        if response.status_code == 202:\n",
        "            print(\"Job submitted successfully.\")\n",
        "            return application_id\n",
        "        else:\n",
        "            print(\"Error submitting job.\")\n",
        "    else:\n",
        "        print(\"Error creating new application.\")\n",
        "\n",
        "    return None\n",
        "\n",
        "def track_resource_usage(resource_manager_url, application_id):\n",
        "    # Track the resource usage of the job\n",
        "    resource_usage_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/appattempts\"\n",
        "\n",
        "    while True:\n",
        "        response = requests.get(resource_usage_url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            app_attempts = data[\"appAttempts\"][\"appAttempt\"]\n",
        "\n",
        "            if isinstance(app_attempts, list):\n",
        "                latest_attempt = app_attempts[-1]\n",
        "                allocated_resources = latest_attempt[\"allocatedResources\"]\n",
        "                memory = allocated_resources[\"memory\"]\n",
        "                vcores = allocated_resources[\"vCores\"]\n",
        "\n",
        "                print(f\"Allocated Memory: {memory} MB\")\n",
        "                print(f\"Allocated vCores: {vcores}\")\n",
        "\n",
        "        time.sleep(5)\n",
        "\n",
        "# Provide the ResourceManager URL\n",
        "resource_manager_url = \"http://localhost:8088\"\n",
        "\n",
        "# Provide the details of the job to be submitted\n",
        "job_name = \"MyHadoopJob\"\n",
        "jar_file = \"path_to_your_hadoop_jar_file\"\n",
        "main_class = \"your_hadoop_main_class\"\n",
        "input_path = \"input_path_in_hdfs\"\n",
        "output_path = \"output_path_in_hdfs\"\n",
        "memory_mb = 2048\n",
        "vcores = 2\n",
        "\n",
        "# Submit the Hadoop job to YARN\n",
        "application_id = submit_yarn_job(resource_manager_url, job_name, jar_file, main_class, input_path, output_path, memory_mb, vcores)\n",
        "\n",
        "if application_id:\n",
        "    # Track the resource usage of the job\n",
        "    track_resource_usage(resource_manager_url, application_id)\n"
      ],
      "metadata": {
        "id": "A1hoBGhpomt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import time\n",
        "\n",
        "\n",
        "class MapReduceJob(MRJob):\n",
        "    def configure_args(self):\n",
        "        super(MapReduceJob, self).configure_args()\n",
        "        self.add_passthru_arg('--split-size', type=int, help='Input split size')\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        # Mapper implementation\n",
        "        # ...\n",
        "\n",
        "    def reducer(self, key, values):\n",
        "        # Reducer implementation\n",
        "        # ...\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper,\n",
        "                   reducer=self.reducer)\n",
        "        ]\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Specify different input split sizes to compare\n",
        "    split_sizes = [100, 1000, 10000]\n",
        "\n",
        "    for split_size in split_sizes:\n",
        "        # Create an instance of the MapReduce job\n",
        "        job = MapReduceJob(args=['--split-size', str(split_size)])\n",
        "\n",
        "        # Start measuring the execution time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run the MapReduce job\n",
        "        with job.make_runner() as runner:\n",
        "            runner.run()\n",
        "\n",
        "        # Calculate the execution time\n",
        "        execution_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Split size: {split_size}\")\n",
        "        print(f\"Execution time: {execution_time} seconds\")\n",
        "        print(\"----------\")\n"
      ],
      "metadata": {
        "id": "ofCTywPmpE4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}