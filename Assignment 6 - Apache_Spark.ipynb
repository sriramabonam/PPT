{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku2PnIMO4Zrh"
      },
      "outputs": [],
      "source": [
        "1. Working with RDDs:\n",
        " a) Write a Python program to create an RDD from a local data source.\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Creation\")\n",
        "\n",
        "# Local data source\n",
        "data = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Create an RDD from the local data source\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Perform operations on the RDD\n",
        "result = rdd.map(lambda x: x * 2).collect()\n",
        "\n",
        "# Print the result\n",
        "for num in result:\n",
        "    print(num)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Transformations and Actions\")\n",
        "\n",
        "# Local data source\n",
        "data = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Create an RDD from the local data source\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Perform transformations and actions on the RDD\n",
        "squared_rdd = rdd.map(lambda x: x ** 2)  # Square each element\n",
        "filtered_rdd = squared_rdd.filter(lambda x: x > 10)  # Filter elements greater than 10\n",
        "sum_result = filtered_rdd.reduce(lambda x, y: x + y)  # Calculate the sum of remaining elements\n",
        "\n",
        "# Print the results\n",
        "print(\"Squared RDD:\")\n",
        "for num in squared_rdd.collect():\n",
        "    print(num)\n",
        "\n",
        "print(\"\\nFiltered RDD:\")\n",
        "for num in filtered_rdd.collect():\n",
        "    print(num)\n",
        "\n",
        "print(\"\\nSum Result:\", sum_result)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "64U5ASCj47sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate\n",
        "\n",
        " from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Operations\")\n",
        "\n",
        "# Local data source\n",
        "data = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Create an RDD from the local data source\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Use RDD operations to analyze and manipulate data\n",
        "squared_rdd = rdd.map(lambda x: x ** 2)  # Square each element\n",
        "even_rdd = squared_rdd.filter(lambda x: x % 2 == 0)  # Filter even numbers\n",
        "sum_result = rdd.reduce(lambda x, y: x + y)  # Calculate the sum of all elements\n",
        "product_result = rdd.reduce(lambda x, y: x * y)  # Calculate the product of all elements\n",
        "mean_result = rdd.mean()  # Calculate the mean of all elements\n",
        "\n",
        "# Print the results\n",
        "print(\"Squared RDD:\")\n",
        "for num in squared_rdd.collect():\n",
        "    print(num)\n",
        "\n",
        "print(\"\\nEven Numbers:\")\n",
        "for num in even_rdd.collect():\n",
        "    print(num)\n",
        "\n",
        "print(\"\\nSum Result:\", sum_result)\n",
        "print(\"Product Result:\", product_result)\n",
        "print(\"Mean Result:\", mean_result)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "HEIOWp5t5Jat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. Spark DataFrame Operations:\n",
        "  a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"CSV to DataFrame\").getOrCreate()\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = spark.read.csv(\"path/to/your/csv/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Display the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Perform further operations on the DataFrame\n",
        "# ...\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "mZZMnd_K5WxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
        " from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrame Operations\").getOrCreate()\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = spark.read.csv(\"path/to/your/csv/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Filter the DataFrame based on a condition\n",
        "filtered_df = df.filter(df[\"age\"] > 30)\n",
        "filtered_df.show()\n",
        "\n",
        "# Group the DataFrame by a column and calculate aggregates\n",
        "grouped_df = df.groupBy(\"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n",
        "grouped_df.show()\n",
        "\n",
        "# Join two DataFrames based on a common column\n",
        "other_df = spark.read.csv(\"path/to/another/csv/file.csv\", header=True, inferSchema=True)\n",
        "joined_df = df.join(other_df, on=\"id\", how=\"inner\")\n",
        "joined_df.show()\n",
        "\n",
        "# Perform additional operations on the DataFrames\n",
        "# ...\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "mo24U5Ae5nyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c) Apply Spark SQL queries on the DataFrame to extract insights from the data\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark SQL Queries\").getOrCreate()\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = spark.read.csv(\"path/to/your/csv/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Register the DataFrame as a temporary table\n",
        "df.createOrReplaceTempView(\"my_table\")\n",
        "\n",
        "# Execute SQL queries on the DataFrame\n",
        "query1 = \"SELECT * FROM my_table WHERE age > 30\"\n",
        "result1 = spark.sql(query1)\n",
        "result1.show()\n",
        "\n",
        "query2 = \"SELECT gender, AVG(salary) AS avg_salary, MAX(age) AS max_age FROM my_table GROUP BY gender\"\n",
        "result2 = spark.sql(query2)\n",
        "result2.show()\n",
        "\n",
        "# Perform additional SQL queries on the DataFrame\n",
        "# ...\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "5BYS4jKr5yzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. Spark Streaming\n",
        "a) Write a Python program to create a Spark Streaming application.\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(appName=\"Spark Streaming Application\")\n",
        "\n",
        "# Create a StreamingContext with a batch interval of 1 second\n",
        "ssc = StreamingContext(sparkContext=sc, batchDuration=1)\n",
        "\n",
        "# Create a DStream by connecting to a TCP socket and listening for data\n",
        "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "# Perform transformations on the DStream\n",
        "words = lines.flatMap(lambda line: line.split(\" \"))\n",
        "word_counts = words.countByValue()\n",
        "\n",
        "# Print the word counts\n",
        "word_counts.pprint()\n",
        "\n",
        "# Start the streaming context\n",
        "ssc.start()\n",
        "\n",
        "# Await termination\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "o8HF2l0657cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(appName=\"Spark Streaming Application\")\n",
        "\n",
        "# Create a StreamingContext with a batch interval of 1 second\n",
        "ssc = StreamingContext(sparkContext=sc, batchDuration=1)\n",
        "\n",
        "# Configure Kafka parameters\n",
        "kafka_params = {\n",
        "    \"bootstrap.servers\": \"localhost:9092\",\n",
        "    \"group.id\": \"my_consumer_group\"\n",
        "}\n",
        "\n",
        "# Consume data from a Kafka topic\n",
        "kafka_topic = \"my_topic\"\n",
        "kafka_stream = KafkaUtils.createDirectStream(ssc, [kafka_topic], kafka_params)\n",
        "\n",
        "# Or, consume data from a socket\n",
        "socket_stream = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "# Perform transformations on the input DStream\n",
        "lines = kafka_stream.map(lambda x: x[1])  # Extract the value from Kafka messages\n",
        "words = lines.flatMap(lambda line: line.split(\" \"))\n",
        "word_counts = words.countByValue()\n",
        "\n",
        "# Print the word counts\n",
        "word_counts.pprint()\n",
        "\n",
        "# Start the streaming context\n",
        "ssc.start()\n",
        "\n",
        "# Await termination\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "xUNglbo_8BW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c) Implement streaming transformations and actions to process and analyze the incoming data stream.\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(appName=\"Streaming Transformations and Actions\")\n",
        "\n",
        "# Create a StreamingContext with a batch interval of 1 second\n",
        "ssc = StreamingContext(sparkContext=sc, batchDuration=1)\n",
        "\n",
        "# Configure Kafka parameters\n",
        "kafka_params = {\n",
        "    \"bootstrap.servers\": \"localhost:9092\",\n",
        "    \"group.id\": \"my_consumer_group\"\n",
        "}\n",
        "\n",
        "# Consume data from a Kafka topic\n",
        "kafka_topic = \"my_topic\"\n",
        "kafka_stream = KafkaUtils.createDirectStream(ssc, [kafka_topic], kafka_params)\n",
        "\n",
        "# Extract and process the incoming data stream\n",
        "lines = kafka_stream.map(lambda x: x[1])  # Extract the value from Kafka messages\n",
        "\n",
        "# Perform transformations and actions on the DStream\n",
        "word_counts = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print the word counts\n",
        "word_counts.pprint()\n",
        "\n",
        "# Perform additional operations on the DStream\n",
        "# ...\n",
        "\n",
        "# Start the streaming context\n",
        "ssc.start()\n",
        "\n",
        "# Await termination\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "hTaNFFCt8Kru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. Spark SQL and Data Source Integration:\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark-Database Integration\").getOrCreate()\n",
        "\n",
        "# Configure the MySQL database connection\n",
        "url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
        "properties = {\n",
        "    \"user\": \"your_username\",\n",
        "    \"password\": \"your_password\",\n",
        "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
        "}\n",
        "\n",
        "# Read data from the database table\n",
        "table_name = \"employees\"\n",
        "df = spark.read.jdbc(url=url, table=table_name, properties=properties)\n",
        "\n",
        "# Display the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Perform further operations on the DataFrame\n",
        "# ...\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "8UeAmBim8UFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " b)Perform SQL operations on the data stored in the database using Spark SQL\n",
        " from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark SQL - Database Integration\").getOrCreate()\n",
        "\n",
        "# Configure the MySQL database connection\n",
        "url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
        "properties = {\n",
        "    \"user\": \"your_username\",\n",
        "    \"password\": \"your_password\",\n",
        "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
        "}\n",
        "\n",
        "# Register the database table as a temporary view\n",
        "table_name = \"employees\"\n",
        "spark.read.jdbc(url=url, table=table_name, properties=properties).createOrReplaceTempView(table_name)\n",
        "\n",
        "# Execute SQL queries on the table\n",
        "query = \"SELECT * FROM employees WHERE age > 30\"\n",
        "result = spark.sql(query)\n",
        "\n",
        "# Display the query result\n",
        "result.show()\n",
        "\n",
        "# Perform additional SQL operations\n",
        "# ...\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "dl9h0ZHr8ieA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3.\n",
        "df = spark.read.text(\"s3a://your_bucket/your_file.txt\")\n",
        "df.write.text(\"s3a://your_bucket/your_output.txt\")"
      ],
      "metadata": {
        "id": "XsC2lFoh8uSL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}